"""CLI: Evaluate a trained agent on the test set.

Usage
-----
    python -m src.cli.evaluate --run_dir runs/20240101_120000_dqn_BTC-USD --symbol BTC-USD
"""

from __future__ import annotations

import argparse
import json
from pathlib import Path

import numpy as np
import pandas as pd

from src.eval.backtester import (
    backtest_agent,
    backtest_sb3,
    save_backtest_results,
)
from src.eval.plots import (
    plot_drawdown,
    plot_equity_curves,
    plot_positions,
    plot_trades_on_price,
)
from src.utils.config import load_yaml
from src.utils.logging import get_logger
from src.utils.paths import data_processed_dir

logger = get_logger(__name__)


def _generate_summary(metrics: dict, run_dir: Path, algo: str) -> None:
    """Write a RESULTS_SUMMARY.md into the run directory."""
    lines = [
        f"# Results Summary — {algo.upper()}",
        "",
        f"**Run directory:** `{run_dir}`",
        "",
        "## Metrics",
        "",
        "| Metric | Value |",
        "| ------ | ----- |",
    ]
    for k, v in metrics.items():
        if isinstance(v, float):
            lines.append(f"| {k} | {v:.6f} |")
        else:
            lines.append(f"| {k} | {v} |")
    lines += [
        "",
        "## Plots",
        "",
        "- `equity_curves.png` — equity curve vs starting capital",
        "- `drawdown.png` — drawdown over time",
        "- `trades_on_price.png` — buy/sell markers on price chart",
        "- `positions.png` — position held over time",
        "",
        "---",
        "*Auto-generated by the evaluation pipeline.*",
    ]
    (run_dir / "RESULTS_SUMMARY.md").write_text("\n".join(lines))


def main(argv: list[str] | None = None) -> None:
    parser = argparse.ArgumentParser(description="Evaluate a trained model on test data")
    parser.add_argument("--run_dir", type=str, required=True, help="Path to run directory")
    parser.add_argument("--symbol", type=str, default="BTC-USD")
    args = parser.parse_args(argv)

    run_dir = Path(args.run_dir)
    if not run_dir.exists():
        raise FileNotFoundError(f"Run directory not found: {run_dir}")

    # Load config snapshot from run
    cfg = load_yaml(run_dir / "config.yaml")
    algo = cfg.get("algorithm", "unknown")
    env_cfg = {**cfg.get("env", {}), **cfg.get("features", {})}

    # Load test data
    proc = data_processed_dir(args.symbol)
    test_df = pd.read_csv(proc / "test.csv", parse_dates=True, index_col=0)
    test_raw = pd.read_csv(proc / "test_raw.csv", parse_dates=True, index_col=0)
    with open(proc / "feature_cols.json") as f:
        feature_cols = json.load(f)

    logger.info(f"[bold]Evaluating {algo.upper()}[/bold] on test set ({len(test_df)} rows)")

    # --- Dispatch based on algorithm ---
    if algo == "qlearning":
        from src.agents.qlearning import QLearningAgent

        agent = QLearningAgent.load(run_dir / "model")
        results = backtest_agent(agent, test_df, feature_cols, env_cfg, cfg.get("risk"))

    elif algo == "dqn":
        from src.agents.dqn import DQNAgent

        obs_dim = test_df[feature_cols].shape[1] * env_cfg.get("window_size", 50) + 3
        agent = DQNAgent.load(run_dir / "model", obs_dim=obs_dim)
        results = backtest_agent(agent, test_df, feature_cols, env_cfg, cfg.get("risk"))

    elif algo == "ppo":
        from src.agents.ppo import load_ppo

        model = load_ppo(run_dir)
        continuous = cfg.get("ppo", {}).get("action_type", "discrete") == "continuous"
        results = backtest_sb3(model, test_df, feature_cols, env_cfg, continuous=continuous)

    else:
        raise ValueError(f"Unknown algorithm '{algo}' in config.")

    # --- Save results ---
    save_backtest_results(results, run_dir, tag=algo)

    # --- Plots ---
    plot_equity_curves({"Agent": results["equity_curve"]}, run_dir)
    plot_drawdown(results["equity_curve"], run_dir)

    if "Close" in test_raw.columns:
        price = test_raw["Close"]
        plot_trades_on_price(price, results["trades"], run_dir)

    if results.get("positions"):
        dates = results["equity_curve"].index
        plot_positions(results["positions"], dates, run_dir)

    _generate_summary(results["metrics"], run_dir, algo)
    logger.info(f"[bold green]Evaluation complete.[/bold green]  See {run_dir}")


if __name__ == "__main__":
    main()
